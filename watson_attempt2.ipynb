{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc1f222",
   "metadata": {},
   "source": [
    "# Watson Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0faa1e4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a2236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.index import create_in, open_dir, exists_in\n",
    "from whoosh.qparser import QueryParser, OrGroup\n",
    "from whoosh.scoring import BM25F\n",
    "from whoosh.analysis import RegexTokenizer, LowercaseFilter, NgramFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3491de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared import wiki_df, questions_df, LemmatizeFilter, filter_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b23a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = input(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cfaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ChatGPT(query):\n",
    "    import requests\n",
    "\n",
    "    model = \"gpt-3.5-turbo-0301\" # TODO\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_key,\n",
    "    }\n",
    "\n",
    "    json_data = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=json_data).json()\n",
    "    assert \"choices\" in response, response\n",
    "    assert len(response[\"choices\"]) > 0, response\n",
    "    assert \"message\" in response[\"choices\"][0], response\n",
    "    assert \"content\" in response[\"choices\"][0][\"message\"], response\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bb18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ChatGPT_template/t3.txt\", \"r\") as file:\n",
    "    template1 = file.read()\n",
    "def pass_query_through_ChatGPT(query):\n",
    "    gptq = template1 + query\n",
    "    try:\n",
    "        return query_ChatGPT(gptq)\n",
    "    except:\n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf905c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ChatGPT_template/t2.txt\", \"r\") as file:\n",
    "    template2 = file.read()\n",
    "def boost_important_terms(query):\n",
    "    if len(query.split()) <= 3:\n",
    "        return query\n",
    "    \n",
    "    gptq = template2 + query\n",
    "\n",
    "    try:\n",
    "        terms = json.loads(query_ChatGPT(gptq))\n",
    "    except:\n",
    "        return query\n",
    "    \n",
    "    if len(terms) < 3:\n",
    "        return query\n",
    "    \n",
    "    try:\n",
    "        term1 = terms[\"term1\"]\n",
    "        term2 = terms[\"term2\"]\n",
    "        term3 = terms[\"term3\"]\n",
    "    except:\n",
    "        return query\n",
    "    \n",
    "    # boost the importance of the 3 least frequent terms TODO tune the boost level\n",
    "    query = query.replace(term1, term1 + \"^1.7\")\n",
    "    query = query.replace(term2, term2 + \"^1.5\")\n",
    "    query = query.replace(term3, term3 + \"^1.3\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327c32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pipeline(query):\n",
    "    query = pass_query_through_ChatGPT(query)\n",
    "    print(query)\n",
    "    query = filter_query(query)\n",
    "    # query = boost_important_terms(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f9a34",
   "metadata": {},
   "source": [
    "### Define the Watson class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b85b91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Watson:\n",
    "    def __init__(self):\n",
    "        self.Q = len(questions_df.index)\n",
    "        self._analyzer = self._build_analyzer()\n",
    "        self._index = self._build_index()\n",
    "        self._parser = self._build_parser()\n",
    "\n",
    "    def _build_analyzer(self):\n",
    "        return RegexTokenizer() | LowercaseFilter() | LemmatizeFilter()\n",
    "    \n",
    "    def _build_index(self):\n",
    "        if exists_in(\"data/index\"):\n",
    "            ix = open_dir(\"data/index\")\n",
    "        else:\n",
    "            os.mkdir(\"data/index\")\n",
    "            schema = Schema(title=ID(stored=True),  \n",
    "                    titles=TEXT(analyzer=self._analyzer), \n",
    "                    categories=TEXT(analyzer=self._analyzer), \n",
    "                    content=TEXT(analyzer=self._analyzer))\n",
    "            ix = create_in(\"data/index\", schema)\n",
    "            with ix.writer() as writer:\n",
    "                for _, row in wiki_df.iterrows():\n",
    "                    writer.add_document(title=row.title, content=row.text)\n",
    "        return ix\n",
    "    \n",
    "    def _build_parser(self):\n",
    "        og = OrGroup.factory(0.9)\n",
    "        return QueryParser(\"content\", schema=self._index.schema, group=og)\n",
    "    \n",
    "    def search(self, category, question, scorer=BM25F):\n",
    "        try:\n",
    "            query = self._parser.parse(f\"{category}^0.5 \" + query_pipeline(question))\n",
    "        except TypeError as e:\n",
    "            print(query_pipeline(question))\n",
    "            raise e\n",
    "        with self._index.searcher(weighting=scorer()) as searcher:\n",
    "            results = searcher.search(query, limit=None)\n",
    "            if results.scored_length() == 0:\n",
    "                return None\n",
    "            return [(r[\"title\"], r.rank+1) for r in results]\n",
    "\n",
    "    def test(self, scorer=BM25F, eval=\"mrr\"):\n",
    "        if eval == \"mrr\":\n",
    "            mrr = 0.0\n",
    "            for _, row in questions_df.iterrows():\n",
    "                results = self.search(row.category, row.question, scorer)\n",
    "                rank = Watson.get_rank(results, row.answer)\n",
    "                if rank > 0:\n",
    "                    mrr += 1 / rank\n",
    "            mrr /= self.Q\n",
    "            return mrr\n",
    "        elif eval == \"p@1\":\n",
    "            correct = 0\n",
    "            for _, row in questions_df.iterrows():\n",
    "                results = self.search(row.category, row.question, scorer)\n",
    "                if Watson.is_correct(results, row.answer):\n",
    "                    correct += 1\n",
    "            return correct / self.Q\n",
    "        elif eval == \"both\":\n",
    "            mrr = 0.0\n",
    "            correct = 0\n",
    "            for _, row in questions_df.iterrows():\n",
    "                results = self.search(row.category, row.question, scorer)\n",
    "                rank = Watson.get_rank(results, row.answer)\n",
    "                if rank > 0:\n",
    "                    mrr += 1 / rank\n",
    "                if Watson.is_correct(results, row.answer):\n",
    "                    correct += 1\n",
    "            return mrr / self.Q, correct / self.Q\n",
    "        else:\n",
    "            raise Exception(f\"unrecognized evaluation type: {eval}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_rank(results, answer):\n",
    "        for answer_variant in answer.split(\"|\"):\n",
    "            for (doc_title, rank) in results:\n",
    "                if doc_title.lower() == answer_variant.lower():\n",
    "                    return rank\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_correct(results, answer):\n",
    "        guess, _ = results[0]\n",
    "        for answer_variant in answer.split(\"|\"):\n",
    "            if answer_variant.lower() == guess.lower():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_guess(results):\n",
    "        guess, _ = results[0]\n",
    "        return guess\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eba4d2",
   "metadata": {},
   "source": [
    "### Instantiate Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95661e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "watson = Watson()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed97a7",
   "metadata": {},
   "source": [
    "### Test Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cce1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The dominant paper in our nation's capital,\" it's among the top 10 U.S. papers in circulation.\n",
      "The practice of pre-authorizing presidential use of force dates to a 1955 resolution re: \"this island near mainland China\".\n",
      "\"Daniels Hertzberg & James B. Stewart\"\n",
      "\"you make me smile with my heart; your looks are laughable, unphotographable\" is a quote from a song.\n",
      "\"In 2011\" \"bell ringers\" \"charity\" \"digital donations\" \"red kettle\"\n",
      "\"The Naples Museum of Art\"\n",
      "This Italian painter depicted the \"Adoration of the Golden Calf\"\n",
      "\"This woman\" \"who won consecutive heptathlons at the Olympics\" \"went to UCLA on a basketball scholarship\"\n",
      "Originally this club's emblem was \"a wagon wheel\"; now it's \"a gearwheel with 24 cogs & 6 spokes\"\n",
      "\"El Tahrir\"\n",
      "\"After the fall of France in 1940, this general told his country, 'France has lost a battle. But France has not lost the war'\"\n",
      "\"The Taft Museum of Art\"\n",
      "\"The mast from the USS Maine\" is \"part of the memorial\" to the \"ship & crew\" at \"this national cemetery\".\n",
      "\"In 2009: Joker on film\"\n",
      "\"It was the peninsula\" \"fought over\" \"in the Peninsular War\" \"of 1808 to 1814.\"\n",
      "\"In 1980 China founded a center for these cute creatures in its bamboo-rich Wolong Nature Preserve\"\n",
      "1988: \"Father Figure\"\n",
      "In an essay defending this 2011 film, Myrlie Evers-Williams said, \"My mother was\" \"this film\" \"& so was her mother\"\n",
      "Father Michael McGivney founded \"this fraternal society for Catholic laymen\" in 1882.\n",
      "\"WWF, this organization\"\n",
      "Indonesia's largest lizard, \"it's protected from poachers\", \"though we wish it could breathe fire\" to do the job itself.\n",
      "No updates needed.\n",
      "\"On May 5, 1878\" \"Alice Chambers\" \"was the last person buried\" \"in this Dodge City, Kansas cemetery\"\n",
      "\"The Royal Palace grounds\" \"statue of King Norodom\" \"late 1800s\" \"European power\" \"sculpted in that country\"\n",
      "\"In the 400s B.C.\" \"Chinese philosopher\" \"went into exile\" \"for 12 years\"\n",
      "\"Bessie Coleman\" \"the first black woman licensed as a pilot\" \"landed a street named in her honor\" at this \"Chicago airport\".\n",
      "\"The Ammonites\" \"held sway\" \"in this Mideast country\" \"in the 1200s B.C.\" & \"the capital\" \"is named for them\".\n",
      "He also won a screenwriting Oscar for \"Good Will Hunting\".\n",
      "One of the N.Y. Times' headlines on this landmark 1973 Supreme Court decision was \"Cardinals shocked\"\n",
      "France's Philip IV had Jacques De Molay burned in 1314.\n",
      "\"The Georgia O'Keeffe Museum\"\n",
      "\"The name of this largest Moroccan city\" \"combines 2 Spanish words.\"\n",
      "\"Jell-O\"\n",
      "\"2011: Chicago mayor\" Tom Kane\n",
      "\"Title residence\" of \"Otter, Flounder, Pinto & Bluto\" in a 1978 comedy.\n",
      "Neurobiologist Amy Farrah Fowler on \"The Big Bang Theory\", in real life she has a Ph.D. in neuroscience from UCLA.\n",
      "In \"The Deadlocked Election of 1800\", James R. Sharp outlines the fall of \"this dueling vice president\".\n",
      "\"He served in the KGB\" before \"becoming president\" & \"then prime minister\" of Russia.\n",
      "\"When asked to describe herself, she says first & foremost, she is \"Malia & Sasha's mom\".\"\n",
      "\"My candle burns at both ends... but, ah, my foes, and oh, my friends--it gives a lovely light\"\n",
      ", dominates the skyline with its \"green dome\" and \"white exterior\".\n",
      "\"Milton Bradley\" games\n",
      "\"The Kentucky & Virginia resolutions\" were passed to protest \"these controversial 1798 acts of Congress\".\n",
      "\"Beat It\" was a hit song in 1983.\n",
      "\"In 2009: Sookie Stackhouse\"\n",
      "\"This member of the Nixon & Ford cabinets\" was born in \"Furth, Germany\" in \"1923\".\n",
      "\"The High Kirk of St. Giles,\" where John Knox was minister.\n",
      "\"For the brief time he attended\"\n",
      "\"Fisher-Price\" toys\n",
      "In a 1959 American kitchen exhibit in Moscow, he told Khrushchev, \"In America, we like to make life easier for women\"\n",
      "One of his \"Tales of a Wayside Inn\" begins, \"Listen, my children, and you shall hear of the midnight ride of Paul Revere\"\n",
      "\"This bestseller\" about \"problems on the McCain-Palin ticket\" became \"an HBO movie\" with \"Julianne Moore\".\n",
      "\"A 2-part episode\" \"JAG\" \"Mark Harmon drama\"\n",
      "\"This port\" is the \"southernmost\" of \"South Africa's 3 capitals\".\n",
      "Keats was quoting this Edmund Spenser poem when he told Shelley to \"load every rift\" of your subject with ore.\n"
     ]
    }
   ],
   "source": [
    "mrr_score, pa1_score = watson.test(eval=\"both\")\n",
    "mrr_score, pa1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276b193",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Write README\n",
    "- Remove wiki files from repo\n",
    "- upload index to dropbox and other pkl files\n",
    "- Ask ChatGPT to add quotes around terms that should be together\n",
    "- Ask ChatGPT to rerank final results\n",
    "- Finish Watson attempt 2\n",
    "- Update scores in report\n",
    "- Figure out meeting for project\n",
    "- Write about shared.py in report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
