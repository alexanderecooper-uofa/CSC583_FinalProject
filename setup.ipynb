{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f00e31d",
   "metadata": {},
   "source": [
    "## Set up the environment and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5931ec",
   "metadata": {},
   "source": [
    "### Install the necessary python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09cb08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: whoosh in /opt/conda/lib/python3.11/site-packages (2.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install whoosh\n",
    "%pip install nltk\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25de0bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24d5aa",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d147d7a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T17:39:40.204587Z",
     "start_time": "2024-03-12T17:39:32.830101Z"
    }
   },
   "outputs": [],
   "source": [
    "from env import env\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16a835",
   "metadata": {},
   "source": [
    "### Parse the wiki files into a dataframe and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075891c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T17:39:40.228588Z",
     "start_time": "2024-03-12T17:39:40.204587Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_wiki_file(content):\n",
    "    titles, text = [], []\n",
    "    start = -1\n",
    "    content = re.sub(r\"\\[\\[File:(.*)\\]\\]\", r\"---File:\\1---\", content) # wrap file links in triple brackets to avoid parsing\n",
    "    content = re.sub(r\"\\[\\[Image:(.*)\\]\\]\", r\"---Image:\\1---\", content) # wrap image links in triple brackets to avoid parsing\n",
    "    for match in re.finditer(\"^\\[\\[(.*)\\]\\]\\n\\n\", content, re.MULTILINE):\n",
    "        titles.append(match.group(1))\n",
    "        if start > -1:\n",
    "            t = re.sub(\"---File:(.*)---\", r\"[[File:\\1]]\", content[start:match.start()])\n",
    "            t = re.sub(\"---Image:(.*)---\", r\"[[Image:\\1]]\", t)\n",
    "            text.append(t)\n",
    "        start = match.end()\n",
    "    text.append(content[start:])\n",
    "    assert len(titles) == len(text)\n",
    "    return list(zip(titles, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c223279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T17:45:32.219153Z",
     "start_time": "2024-03-12T17:43:07.671338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006</td>\n",
       "      <td>Continuum Hypothesis</td>\n",
       "      <td>CATEGORIES: Forcing (mathematics), Independenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006</td>\n",
       "      <td>Çevik Bir</td>\n",
       "      <td>CATEGORIES: 1939 births, People from Izmir, Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006</td>\n",
       "      <td>Collectivism</td>\n",
       "      <td>CATEGORIES: Collectivism, Collaboration, Corpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006</td>\n",
       "      <td>Nepeta</td>\n",
       "      <td>CATEGORIES: Lamiaceae, Flora of Africa, Flora ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0006</td>\n",
       "      <td>Cumin</td>\n",
       "      <td>CATEGORIES: Edible Apiaceae, Medicinal plants ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_id                 title  \\\n",
       "0    0006  Continuum Hypothesis   \n",
       "1    0006             Çevik Bir   \n",
       "2    0006          Collectivism   \n",
       "3    0006                Nepeta   \n",
       "4    0006                 Cumin   \n",
       "\n",
       "                                                text  \n",
       "0  CATEGORIES: Forcing (mathematics), Independenc...  \n",
       "1  CATEGORIES: 1939 births, People from Izmir, Li...  \n",
       "2  CATEGORIES: Collectivism, Collaboration, Corpo...  \n",
       "3  CATEGORIES: Lamiaceae, Flora of Africa, Flora ...  \n",
       "4  CATEGORIES: Edible Apiaceae, Medicinal plants ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dir = \"./raw_data/wiki\" # this directory is only stored locally\n",
    "data = {\"file_id\": [], \"title\": [], \"text\": []}\n",
    "redirects = {\"file_id\": [], \"title\": [], \"text\": [], \"redirect\": []}\n",
    "for fname in [f for f in os.listdir(wiki_dir) if not f.startswith(\"._\")]:\n",
    "    with open(os.path.join(wiki_dir, fname), \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    fid = fname.split(\"-\")[-1].replace(\".txt\", \"\")\n",
    "    for title, text in parse_wiki_file(content):\n",
    "        if text.startswith(\"#REDIRECT\"):\n",
    "            redirect = re.sub(\"\\[tpl\\].*\\[/tpl\\]\", \"\", text.replace(\"#REDIRECT\", \"\")).strip()\n",
    "            redirects[\"file_id\"].append(fid)\n",
    "            redirects[\"title\"].append(title.title())\n",
    "            redirects[\"text\"].append(text)\n",
    "            redirects[\"redirect\"].append(redirect)\n",
    "        else:\n",
    "            data[\"file_id\"].append(fid)\n",
    "            data[\"title\"].append(title.title())\n",
    "            data[\"text\"].append(text)\n",
    "wiki_df = pd.DataFrame(data).drop_duplicates(subset=\"title\")\n",
    "wiki_redirects_df = pd.DataFrame(redirects).drop_duplicates(subset=(\"title\", \"redirect\"))\n",
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8f23ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>redirect</th>\n",
       "      <th>redirect_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006</td>\n",
       "      <td>Capitalist</td>\n",
       "      <td>#REDIRECT Capitalism\\n\\n\\n</td>\n",
       "      <td>Capitalism</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006</td>\n",
       "      <td>Cross Cutting</td>\n",
       "      <td>#REDIRECT Cross-cutting\\n\\n\\n</td>\n",
       "      <td>Cross-cutting</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006</td>\n",
       "      <td>Monetary Policy Of Central Banks</td>\n",
       "      <td>#REDIRECT Monetary policy\\n\\n\\n</td>\n",
       "      <td>Monetary policy</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006</td>\n",
       "      <td>Cryptography/Hashfunction</td>\n",
       "      <td>#REDIRECT Hash function\\n\\n\\n</td>\n",
       "      <td>Hash function</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0006</td>\n",
       "      <td>Cryptography/Key</td>\n",
       "      <td>#REDIRECT Key (cryptography)\\n\\n\\n</td>\n",
       "      <td>Key (cryptography)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_id                             title  \\\n",
       "0    0006                        Capitalist   \n",
       "1    0006                     Cross Cutting   \n",
       "2    0006  Monetary Policy Of Central Banks   \n",
       "3    0006         Cryptography/Hashfunction   \n",
       "4    0006                  Cryptography/Key   \n",
       "\n",
       "                                 text            redirect  redirect_index  \n",
       "0          #REDIRECT Capitalism\\n\\n\\n          Capitalism              -1  \n",
       "1       #REDIRECT Cross-cutting\\n\\n\\n       Cross-cutting              -1  \n",
       "2     #REDIRECT Monetary policy\\n\\n\\n     Monetary policy              -1  \n",
       "3       #REDIRECT Hash function\\n\\n\\n       Hash function              -1  \n",
       "4  #REDIRECT Key (cryptography)\\n\\n\\n  Key (cryptography)              -1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_lookups = {title: index for title, index in zip(wiki_df[\"title\"], wiki_df.index)}\n",
    "assert len(wiki_lookups) == len(wiki_df.index)\n",
    "\n",
    "# for each redirect, find the index of the redirected page for faster lookup\n",
    "def find_redirect_index(redirect):\n",
    "    if redirect not in wiki_lookups:\n",
    "        return -1 # TODO consider searching redirect on wikipedia to find real title\n",
    "    else:\n",
    "        return wiki_lookups[redirect]\n",
    "    \n",
    "redirect_indexes = [find_redirect_index(redirect) for redirect in wiki_redirects_df[\"redirect\"]]\n",
    "wiki_redirects_df[\"redirect_index\"] = redirect_indexes\n",
    "wiki_redirects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35414787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64211, 53084)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.to_pickle(f\"{env.data_dir}/wiki.pkl\")\n",
    "wiki_redirects_df.to_pickle(f\"{env.data_dir}/wiki_redirects.pkl\")\n",
    "\n",
    "len(wiki_df.index), len(wiki_redirects_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b121799",
   "metadata": {},
   "source": [
    "### Parse the [questions.txt](./data/questions.txt) file into a dataframe and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d91846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWSPAPERS</td>\n",
       "      <td>The dominant paper in our nation's capital, it...</td>\n",
       "      <td>The Washington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OLD YEAR'S RESOLUTIONS</td>\n",
       "      <td>The practice of pre-authorizing presidential u...</td>\n",
       "      <td>Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWSPAPERS</td>\n",
       "      <td>Daniel Hertzberg &amp; James B. Stewart of this pa...</td>\n",
       "      <td>The Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BROADWAY LYRICS</td>\n",
       "      <td>Song that says, \"you make me smile with my hea...</td>\n",
       "      <td>My Funny Valentine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POTPOURRI</td>\n",
       "      <td>In 2011 bell ringers for this charity started ...</td>\n",
       "      <td>The Salvation Army|Salvation Army</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 category                                           question  \\\n",
       "0              NEWSPAPERS  The dominant paper in our nation's capital, it...   \n",
       "1  OLD YEAR'S RESOLUTIONS  The practice of pre-authorizing presidential u...   \n",
       "2              NEWSPAPERS  Daniel Hertzberg & James B. Stewart of this pa...   \n",
       "3         BROADWAY LYRICS  Song that says, \"you make me smile with my hea...   \n",
       "4               POTPOURRI  In 2011 bell ringers for this charity started ...   \n",
       "\n",
       "                              answer  \n",
       "0                The Washington Post  \n",
       "1                             Taiwan  \n",
       "2            The Wall Street Journal  \n",
       "3                 My Funny Valentine  \n",
       "4  The Salvation Army|Salvation Army  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./raw_data/questions.txt\", \"r\") as file:\n",
    "    questions = file.read().rstrip(\"\\n\").split(\"\\n\\n\")\n",
    "data = {\"category\": [], \"question\": [], \"answer\": []}\n",
    "for question in questions:\n",
    "    question = question.split(\"\\n\")\n",
    "    data[\"category\"].append(question[0])\n",
    "    data[\"question\"].append(question[1])\n",
    "    data[\"answer\"].append(question[2].title())\n",
    "questions_df = pd.DataFrame(data)\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62aabc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.to_pickle(f\"{env.data_dir}/questions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6b28a",
   "metadata": {},
   "source": [
    "### Count the frequency of each term in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae73efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(wiki_df.text)\n",
    "terms = count_vectorizer.get_feature_names_out()\n",
    "total_counts = np.asarray(np.sum(count_matrix, axis=0)).reshape(-1)\n",
    "term_counts = {}\n",
    "for term, count in zip(terms, total_counts):\n",
    "    term = lemmatizer.lemmatize(term.lower())\n",
    "    if term in term_counts:\n",
    "        term_counts[term] += count\n",
    "    else:\n",
    "        term_counts[term] = count\n",
    "\n",
    "del count_matrix, total_counts, terms, count_vectorizer # clean up memory\n",
    "\n",
    "with open(f\"{env.data_dir}/term_counts.pkl\", \"wb\") as file:\n",
    "    pickle.dump(term_counts, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
